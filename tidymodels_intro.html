<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Intro to tidymodels with nflscrapR play-by-play</title>
    <meta charset="utf-8" />
    <meta name="author" content="Thomas Mock" />
    <meta name="date" content="2020-05-21" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Intro to <code>tidymodels</code> with <code>nflscrapR</code> play-by-play
### Thomas Mock
### 2020-05-21

---










### Personas

* You're interested in fitting models in R with NFL data
* You've done some modeling
* You're new to `tidymodels`
* You're awesome because you came to HANIC!

### Slide Link: [`jthomasmock.github.io/nfl_hanic/`](https://jthomasmock.github.io/nfl_hanic/)

### Repo w/ Code: [github.com/jthomasmock/nfl_hanic](https://github.com/jthomasmock/nfl_hanic)

---

# `tidymodels`

The `tidymodels` framework is a collection of packages for modeling and machine learning using `tidyverse` principles.

## Packages
* `rsample`: efficient data splitting and resampling
* `parsnip`: tidy, unified interface to models
* `recipes`: tidy interface to data pre-processing tools for feature engineering
* `workflows`: bundles your pre-processing, modeling, and post-processing
* `tune`: helps optimize the hyperparameters and pre-processing steps
* `yardstick`: measures the performance metrics
* `broom`: converts common R statistical objects into predictable formats
* `dials`: creates and manages tuning parameters/grids

---

# Learn more

* [`tidymodels`.org](https://www.tidymodels.org/learn/) has step by step guides of various complexities

* I'll be adding additional NFL-focused examples at: [TheMockup.blog](https://themockup.blog/posts/2020-05-01-tidy-long-models/)

* Julia Silge's (a `tidymodels` maintainer) [blog](https://juliasilge.com/) or [video series](https://www.youtube.com/channel/UCTTBgWyJl2HrrhQOOc710kA)

* Alison Hill's [Workshop from rstudio::conf2020](https://conf20-intro-ml.netlify.app/)

* Gentle Intro to `tidymodels` on [RStudio RViews blog](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

---


## Before we get started, thank you to:

.pull-left[
### The `nflscrapR` team:
(data from 2009-current)
* Maksim Horowitz
* Ron Yurko
* Sam Ventura
* Rishav Dutta

### The `nflfastR` team:
(data from 2000-current)
* Ben Baldwin
* Sebastian Carl
]

.pull-right[
&lt;img src="https://pbs.twimg.com/profile_images/1076883081192263680/hj53KzOl_400x400.jpg" width="50%" /&gt;
&lt;br&gt;

&lt;img src="https://raw.githubusercontent.com/mrcaseb/nflfastR/master/man/figures/logo.png" width="50%" /&gt;

]

These datasets set the bar for publicly-available NFL data!

---

### The Dataset

Filtered down from the `nflscrapR` and `nflfastR` datasets (~2.25 GB) to only non-penalty run and pass plays from 2000 and 2019. This is about 15 million data points across ~630 thousand plays.

.small[

```r
# Rows: 628,602
# Columns: 25
# $ play_type                  &lt;fct&gt; run, pass, pass, run, run, run, pass, pass, pass, run, pass,…
# $ yards_gained               &lt;dbl&gt; 2, 0, 0, 8, 3, -1, 15, 11, 0, 3, 0, -1, 5, 7, 0, -1, 13, 1, …
# $ season                     &lt;fct&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, …
# $ ydstogo                    &lt;dbl&gt; 10, 8, 8, 10, 2, 10, 11, 10, 10, 10, 7, 10, 11, 6, 10, 10, 1…
# $ posteam                    &lt;fct&gt; ARI, ARI, ARI, NYG, NYG, NYG, NYG, NYG, NYG, NYG, NYG, ARI, …
# $ posteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ defteam                    &lt;fct&gt; NYG, NYG, NYG, ARI, ARI, ARI, ARI, ARI, ARI, ARI, ARI, NYG, …
# $ shotgun                    &lt;fct&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, …
# $ game_id                    &lt;dbl&gt; 2000090300, 2000090300, 2000090300, 2000090300, 2000090300, …
# $ game_date                  &lt;dbl&gt; 11203, 11203, 11203, 11203, 11203, 11203, 11203, 11203, 1120…
# $ game_seconds_remaining     &lt;dbl&gt; 3600, 3544, 3537, 3520, 3520, 3520, 3520, 3520, 3359, 3359, …
# $ down                       &lt;fct&gt; 1, 2, 3, 1, 2, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, …
# $ yardline_100               &lt;dbl&gt; 65, 63, 63, 78, 70, 67, 68, 53, 42, 42, 39, 94, 95, 90, 83, …
# $ qtr                        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
# $ no_huddle                  &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ posteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …
# $ defteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …
# $ score_differential         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ defteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ week                       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
# $ in_red_zone                &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ in_fg_range                &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
# $ total_runs                 &lt;dbl&gt; 0, 1, 1, 0, 1, 2, 3, 3, 3, 3, 4, 1, 2, 2, 3, 3, 4, 4, 5, 6, …
# $ total_pass                 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 2, 3, 3, 2, 2, 3, 3, 4, 4, 5, 5, 5, …
# $ previous_play              &lt;fct&gt; First play of Drive, run, pass, First play of Drive, run, ru…
```

]

---

### Data Prep

I don't want to get lost in the weeds on data prep or feature engineering, but for future reference here are the `filter` and `mutate` steps I applied. I also use `dbplyr` to pull the data in so that I could work with the 2.2 GB of data on disk and only pull into memory the specific rows/columns I wanted to work with. Full details about `dbplyr` + `nflfastR` data on my [blog](https://themockup.blog/posts/2019-04-28-nflfastr-dbplyr-rsqlite/).

.small[

```r
pbp_db &lt;- tbl(DBI::dbConnect(RSQLite::SQLite(), "data/pbp_db.sqlite"), "pbp_clean_2000-2019")


raw_plays &lt;- pbp_db %&gt;% 
  select(game_id, game_date, game_seconds_remaining, season_type, week,season,
         play_type, yards_gained, ydstogo, down, yardline_100, qtr, posteam, posteam_score, defteam, 
         defteam_score,score_differential, shotgun,  no_huddle, 
         posteam_timeouts_remaining, defteam_timeouts_remaining, penalty) %&gt;% 
  filter(play_type %in% c("run", "pass"), 
         penalty == 0,
         season_type == "REG", 
         !is.na(down),
         !is.na(yardline_100))%&gt;% 
  mutate(in_red_zone = if_else(yardline_100 &lt;= 20, 1, 0),
         in_fg_range = if_else(yardline_100 &lt;= 35, 1, 0),
         run = if_else(play_type == "run", 1, 0),
         pass = if_else(play_type == "pass", 1, 0))
```
]
---

### Feature Engineering

I added a few features, namely a running total of number of runs/passes pre-snap and what the previous play was.

.small[

```r
add_previous_play &lt;- raw_plays %&gt;%
  group_by(game_id, posteam) %&gt;%
  mutate(
    total_runs = if_else(play_type == "run",
      cumsum(run) - 1, cumsum(run)
    ),
    total_pass = if_else(play_type == "pass",
      cumsum(pass) - 1, cumsum(pass)
    ),
    previous_play = if_else(posteam == lag(posteam),
      lag(play_type), "First play of Drive"
    ),
    previous_play = if_else(is.na(previous_play),
      replace_na("First play of Drive"), previous_play
    )
  ) %&gt;%
  select(-run, -pass, -penalty, -season_type) %&gt;%
  ungroup() %&gt;%
  collect()

all_plays &lt;- raw_plays %&gt;% 
  mutate_at(vars(
    play_type, season, posteam, defteam, shotgun, down, qtr, no_huddle,
    posteam_timeouts_remaining, defteam_timeouts_remaining, in_red_zone,
    in_fg_range, previous_play
), as.factor)
```

]
---

## Data Splitting w/ `rsample`

Now that I've created the dataset to use, I'll start with `tidymodels` proper.

Do the initial split and stratify by play type (make sure there are equal ratios of run vs pass in `test` and `train`)

```r
split_pbp &lt;- initial_split(all_plays, 0.75, strata = play_type)

split_pbp

# separate the training data
train_data &lt;- training(split_pbp)

# separate the testing data
test_data &lt;- testing(split_pbp)
```

---

### Test vs Train

Split into `training` and `testing` and then confirm the ratios.
.medium[

```r
train_data %&gt;% 
  count(play_type) %&gt;% 
  mutate(ratio = n/sum(n))
```


```r
# A tibble: 2 x 3
#   play_type      n ratio
#   &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;
# 1 pass      270979 0.575
# 2 run       200473 0.425
```



```r
test_data %&gt;% 
  count(play_type) %&gt;% 
  mutate(ratio = n/sum(n))
```


```r
# A tibble: 2 x 3
#   play_type     n ratio
#   &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;
# 1 pass      90326 0.575
# 2 run       66824 0.425
```

]
---

## Add recipe steps with `recipes`

`recipe` steps are changes we make to the dataset, including things like centering, dummy encoding, update columns as ID only, or even custom feature engineering. 



```r
pbp_rec &lt;- recipe(play_type ~ ., data = train_data) %&gt;% 
  # ignore these vars for train/test, but include in data as ID
  update_role(game_id, game_date, yards_gained, new_role = "ID") %&gt;% 
   # removes vars that have large absolute correlations w/ other vars
  step_corr(all_numeric(), threshold = 0.7) %&gt;% 
  step_center(all_numeric()) %&gt;%  # substract mean from numeric
  step_zv(all_predictors()) # remove zero-variance predictors
```

---

## Choose a model and start your engines!

`parsip` supplies a general modeling interface to the wide world of R models!



```r
lr_mod &lt;- logistic_reg(mode = "classification") %&gt;% 
  set_engine("glm")
```

---

## Combine into a `workflow`

We can now combine the model and recipe into a `workflow` - this allows us to define exactly what model and data are going into our `fit`/train call.


```r
lr_wflow &lt;- workflow() %&gt;% 
  add_model(lr_mod) %&gt;% # parsnip model
  add_recipe(pbp_rec)   # recipe from recipes
```

### What is a `workflow`?

A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. For example, if you have a `recipe` and `parsnip` model, these can be combined into a workflow. The advantages are:

* You don’t have to keep track of separate objects in your workspace.

* The recipe prepping and model fitting can be executed using a single call to `fit()`.

* If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with `tune`.

---

## Fit/train the model with `parsnip`

Now we can move forward with fitting/training the model - this is really a one-liner.


```r
pbp_fit_lr &lt;- lr_wflow %&gt;% 
  fit(data = train_data) # fit the model against the training data
```


---

## Predict outcomes with `parsnip`

After the model has been trained we can compare the training data against the holdout testing data.


```r
pbp_pred_lr &lt;- predict(pbp_fit_lr, test_data) %&gt;% 
  # Get probabilities for the class for each observation
  bind_cols(predict(pbp_fit_lr, test_data, type = "prob")) %&gt;% 
  # Add back a "truth" column for what the actual play_type was
  bind_cols(test_data %&gt;% select(play_type)) 
```

---

### Check outcomes with `yardstick`

For confirming how well the model predicts, we can use `yardstick` to plot ROC curves, get AUC and collect general metrics.
.pull-left[
.small[

```r
pbp_pred_lr %&gt;% 
  # get Area under Curve
  roc_auc(truth = play_type, 
          .pred_pass)

pbp_pred_lr %&gt;% 
  # collect and report metrics
  metrics(truth = play_type, 
          .pred_class)
```


```r
# A tibble: 1 x 3
#   .metric .estimator .estimate
#   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
# 1 roc_auc binary         0.761

# A tibble: 2 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.698
# 2 kap      binary         0.384
```
]

]

  
.small[

```r
pbp_pred_lr %&gt;% 
  # calculate ROC curve
  roc_curve(truth = play_type, 
            .pred_pass) %&gt;% 
  # ggplot2 autoplot for AB line 
  # and the path of ROC curve
  autoplot()
```

]

![](lr_roc_curve.png)


---

#### All together!

.small[

```r
# Split
split_pbp &lt;- initial_split(all_plays, 0.75, strata = play_type)

# Split into test/train
train_data &lt;- training(split_pbp)
test_data &lt;- testing(split_pbp)
```


```r
# Create a pre-processing recipe
pbp_rec &lt;- recipe(play_type ~ ., data = train_data) %&gt;% 
  update_role(game_id, game_date, yards_gained, new_role = "ID")

# Choose a model and an engine
lr_mod &lt;- logistic_reg(mode = "classification") %&gt;% 
  set_engine("glm")
```



```r
# Combine the mode and recipe to the workflow
lr_wflow &lt;- workflow() %&gt;% 
  add_model(lr_mod) %&gt;% 
  add_recipe(pbp_rec)
           
# Fit/train the model
pbp_fit_lr &lt;- lr_wflow %&gt;% 
  fit(data = train_data)
```


```r
# Get predictions
pbp_pred_lr &lt;- predict(pbp_fit_lr, test_data) %&gt;% 
  bind_cols(predict(pbp_fit_lr, test_data, type = "prob")) %&gt;% 
  bind_cols(test_data %&gt;% select(play_type)) 

# Check metrics
pbp_pred_lr %&gt;% 
  metrics(truth = play_type, .pred_class)
```

]

---

## Change the model

How about a Random Forest model?


```r
rf_mod &lt;- rand_forest(trees = 1000) %&gt;% 
  set_engine("ranger", 
             # This lets us do variable importance later
             importance = "impurity", 
             # Parallelization of the model
             num.threads = 4) %&gt;% 
  set_mode("classification")

rf_wflow &lt;- workflow() %&gt;% 
  add_model(rf_mod) %&gt;%  # New model
  add_recipe(pbp_rec) # Same recipe

pbp_fit_rf &lt;- rf_wflow %&gt;% 
  fit(data = train_data)

pbp_pred_rf &lt;- predict(pbp_fit_rf, test_data) %&gt;% 
  bind_cols(test_data %&gt;% select(play_type, down, ydstogo)) %&gt;% 
  bind_cols(predict(pbp_fit_rf, test_data, type = "prob"))
```


---

### Quick comparison

The random forest model slightly outperforms the logistic regression, although both are fairly `meh`


```r
pbp_pred_rf %&gt;% 
  metrics(truth = play_type, .pred_class)

pbp_pred_lr %&gt;% 
  metrics(truth = play_type, .pred_class)
```


```r
# A tibble: 2 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.722
# 2 kap      binary         0.433
# 
# # A tibble: 2 x 3
#   .metric  .estimator .estimate
#   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
# 1 accuracy binary         0.700
# 2 kap      binary         0.388
```


---

### Feature Importance

.small[

```r
pbp_fit_rf %&gt;%   
  pull_workflow_fit() %&gt;% 
  vip(num_features = 20)
```

]
![](import_plot.png)


---

### Comparing Models Together

.small[
.pull-left[

```r
roc_rf &lt;- pbp_pred_rf %&gt;% 
  roc_curve(truth = play_type, .pred_pass) %&gt;% 
  mutate(model = "Random Forest")

roc_lr &lt;- pbp_pred_lr %&gt;% 
  roc_curve(truth = play_type, .pred_pass) %&gt;% 
  mutate(model = "Logistic Regression")

full_plot &lt;- bind_rows(roc_rf, roc_lr) %&gt;% 
  ggplot(aes(x = 1 - specificity, 
             y = sensitivity, 
             color = model)) + 
  geom_path(lwd = 1, alpha = 0.5) +
  geom_abline(lty = 3) + 
  scale_color_manual(values = c("#374785", "#E98074")) +
  theme(legend.position = "top")
  
full_plot
```

]

]

.pull-right[

![](combo_plot.png)

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create();
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
